{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A - Intro to Data Science: Project (WIP)\n",
    "## Predicting Loan Outcomes\n",
    "## Group: Andrew Greene and David Modjeska\n",
    "### Harvard University, Fall 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import matplotlib\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy as sp\n",
    "import sklearn.preprocessing as Preprocessing\n",
    "\n",
    "from itertools import combinations\n",
    "from sklearn.cross_validation import KFold as kfold\n",
    "from sklearn.cross_validation import train_test_split as sk_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.linear_model import LinearRegression as Lin_Reg\n",
    "from sklearn.linear_model import LogisticRegression as Log_Reg\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.qda import QDA\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper function to select the columns of interest from the data set\n",
    "def Select_Data(data):\n",
    "    \n",
    "    # list columns to select\n",
    "    features_to_select = [\"loan_status\", \"annual_inc\", \"earliest_cr_line\", \"delinq_2yrs\", \\\n",
    "                          \"emp_length\", \"home_ownership\", \"inq_last_6mths\", \"loan_amnt\", \\\n",
    "                         \"purpose\", \"open_acc\", \"total_acc\", \"term\", \"installment\", \\\n",
    "                         \"revol_bal\", \"sub_grade\", \"issue_d\"]\n",
    "    ratios_to_select = [\"dti\", \"revol_util\"]\n",
    "    text_to_select = [\"desc\"]\n",
    "\n",
    "    # concatenate selected columns\n",
    "    data_select = pd.concat(( \\\n",
    "                            data[features_to_select],\n",
    "                            data[ratios_to_select], \\\n",
    "                            data[text_to_select]), \\\n",
    "                            axis = 1)\n",
    "\n",
    "    # synthesize new columns, and drop temporary columns\n",
    "    monthly_inc = (data[\"annual_inc\"] / 12)\n",
    "    data_select[\"ipr\"] = data[\"installment\"] / monthly_inc # income to payment ratio\n",
    "    data_select[\"rir\"] = data[\"revol_bal\"] / monthly_inc # revolving to income ratio\n",
    "    data_select = data_select.drop(\"installment\", axis = 1)\n",
    "    data_select = data_select.drop(\"revol_bal\", axis = 1)\n",
    "    \n",
    "    # rename columns for legibility\n",
    "    data_select.columns = [\n",
    "        \"loan_status\", \"annual_income\", \"earliest_credit\", \"delinq_2_yrs\", \\\n",
    "        \"employ_length\", \"home_owner\", \"inquiry_6_mos\", \"loan_amount\", \\\n",
    "        \"loan_purpose\", \"open_accounts\", \"total_acccounts\", \"loan_term\", \\\n",
    "        \"loan_subgrade\", \"issue_date\", \"dti\", \"revol_util\", \"description\", \\\n",
    "        \"ipr\", \"rir\" \\\n",
    "    ]\n",
    "    \n",
    "    return data_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper function to filter the data set down to rows of interest\n",
    "def Filter_Data(data_select):\n",
    "    \n",
    "    # set flags for resolved loans\n",
    "    status_indexes = (data_select[\"loan_status\"] == \"Fully Paid\") | \\\n",
    "                    (data_select[\"loan_status\"] == \"Charged Off\")\n",
    "\n",
    "    # set flags for date range of interest\n",
    "    earliest_date = pd.to_datetime(\"2012-01-01\")\n",
    "    data_my = pd.to_datetime(data_select[\"issue_date\"])\n",
    "    date_indexes = (data_my > earliest_date)\n",
    "\n",
    "    # filter rows per flags of interest\n",
    "    data_filter = data_select.ix[status_indexes & date_indexes, :].reset_index()\n",
    "    data_filter = data_filter.drop(\"issue_date\", axis = 1)\n",
    "    \n",
    "    return data_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper function to clean data - recoding, retyping, pruning, and censoring\n",
    "def Clean_Data(data_filter):\n",
    "    data_clean = data_filter.copy()\n",
    "\n",
    "    # recode loan status as boolean: fully paid = True\n",
    "    data_clean[\"loan_status\"] = data_clean[\"loan_status\"] == \"Fully Paid\"\n",
    "\n",
    "    # recode loan subgrades from 1 (best) to 35 (worst)\n",
    "    num_grades = 5\n",
    "    grade = data_clean[\"loan_subgrade\"].str[0]\n",
    "    grade = (pd.DataFrame(ord(c) for c in grade) - ord('A')) * num_grades\n",
    "    sub_grade = data_clean[\"loan_subgrade\"].str[1].astype('int')\n",
    "    data_clean[\"loan_subgrade\"] =  grade + sub_grade\n",
    "\n",
    "    # convert earliest credit date to datetime\n",
    "    data_clean[\"earliest_credit\"] = pd.to_datetime(data_clean[\"earliest_credit\"])\n",
    "\n",
    "    # prune extra text in loan term \n",
    "    data_clean[\"loan_term\"] = data_clean[\"loan_term\"].str.replace(\" months\", \"\")\n",
    "\n",
    "    # prune extra text in employment length, and right-censor\n",
    "    data_clean[\"employ_length\"] = data_clean[\"employ_length\"].str.replace(\" years*\", \"\")\n",
    "    data_clean[\"employ_length\"] = data_clean[\"employ_length\"].str.replace(\"10\\+\", \"10\")\n",
    "    data_clean[\"employ_length\"] = data_clean[\"employ_length\"].str.replace(\"< 1\", \"0\")\n",
    "    \n",
    "    # right-censor delinquencies and inquiries\n",
    "    data_clean[\"delinq_2_yrs\"] = np.clip(data_clean[\"delinq_2_yrs\"], 0, 2)\n",
    "    data_clean[\"inquiry_6_mos\"] = np.clip(data_clean[\"inquiry_6_mos\"], 0, 3)\n",
    "    \n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper function to clean data - filtering nuisance NaNs (not structural NaNs)\n",
    "def Clean_Data2(data_clean):\n",
    "    n, p = data_clean.shape\n",
    "    \n",
    "    # count nulls by column\n",
    "    col_nan_pct = data_clean.isnull().sum() / n\n",
    "    \n",
    "    # flag columns that have some nuisance nulls\n",
    "    cols_with_nans = (col_nan_pct > 0.0) & (col_nan_pct < 0.01)\n",
    "    \n",
    "    # flag rows that have some nuisance nulls in the flagged columns\n",
    "    rows_without_nans_flags = data_clean.ix[:, cols_with_nans].notnull()\n",
    "    \n",
    "    # index the flagged rows with some nuisance nulls\n",
    "    rows_without_nans_indexes = np.where(rows_without_nans_flags)\n",
    "    \n",
    "    # filter the data set to rows with no nuisance nulls\n",
    "    data_clean2 = data_clean.ix[rows_without_nans_indexes[0], :]\n",
    "    \n",
    "    return data_clean2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper function to pre-process each data subset to get around memory limits\n",
    "def Prep_Data_Part(index, num_parts, file_prefix, data_all):\n",
    "    filename = file_prefix + str(index) + \".csv\"\n",
    "    \n",
    "    # pre-process new data part if the file doesn't already exist\n",
    "    if not op.isfile(filename):\n",
    "        n, p = data_all.shape\n",
    "        \n",
    "        # pre-process the row range for this data part, avoiding empty data subsets\n",
    "        start_row = index * (n / num_parts)\n",
    "        data_part = data_all.ix[range(start_row, start_row + (n / num_parts)), :]\n",
    "        if data_part.shape[0] > 0:\n",
    "            data_select = Select_Data(data_part)\n",
    "            data_filter = Filter_Data(data_select)\n",
    "            if data_filter.shape[0] > 0:\n",
    "                data_clean = Clean_Data(data_filter)\n",
    "                data_clean2 = Clean_Data2(data_clean)\n",
    "                data_clean2.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper function to pre-process full data set and save new file, \n",
    "# or to read pre-processed file if it already exists\n",
    "def Preprocess_Full_Dataset():\n",
    "    file_prefix = \"./data_parts/loan_clean_part\"\n",
    "    full_clean_data_file = \"loan_clean_data.csv\"\n",
    "    num_parts = 30\n",
    "\n",
    "    # pre-process data set and save result as new file\n",
    "    if not op.isfile(full_clean_data_file):\n",
    "\n",
    "        # pre-process and save part files\n",
    "        data_raw = pd.read_csv(\"loan.csv\")\n",
    "        for part in range(num_parts):\n",
    "            Prep_Data_Part(part, num_parts, file_prefix, data_raw)       \n",
    "\n",
    "        # read and concatenate part files\n",
    "        data = pd.DataFrame({})\n",
    "        for part in range(num_parts):\n",
    "            file_part = file_prefix + str(part) + \".csv\"\n",
    "            if op.isfile(file_part):\n",
    "                data_part = pd.read_csv(file_part)\n",
    "                data = pd.concat((data, data_part), axis = 0)\n",
    "\n",
    "        # save full file\n",
    "        data = data.reset_index()\n",
    "        data.to_csv(full_clean_data_file, index = False)\n",
    "\n",
    "    # read pre-processed full data file\n",
    "    else:\n",
    "        data = pd.read_csv(full_clean_data_file)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to pre-process sampled data set and save new file, \n",
    "# or to read pre-processed file if it already exists\n",
    "def Preprocess_Sample_Dataset():\n",
    "    sample_percent = 10\n",
    "    sample_clean_data_file = \"loan_clean_data_\" + str(sample_percent) + \"pct.csv\"\n",
    "\n",
    "    # pre-process sample data set and save result as new file\n",
    "    if not op.isfile(sample_clean_data_file):\n",
    "        data_raw = pd.read_csv(\"loan.csv\")\n",
    "        data_sample, data_other = sk_split(data_raw, train_size = sample_percent / 100.0)\n",
    "        data_select = Select_Data(data_sample)\n",
    "        data_filter = Filter_Data(data_select)\n",
    "        data_clean = Clean_Data(data_filter)\n",
    "        data_clean2 = Clean_Data2(data_clean)\n",
    "        data_clean2.to_csv(sample_clean_data_file, index = False)\n",
    "\n",
    "    # read pre-processed sample data file\n",
    "    else:\n",
    "        data_clean2 = pd.read_csv(sample_clean_data_file)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmodjeska/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2825: DtypeWarning: Columns (19,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "# create or load appropriate version of data set for analysis\n",
    "\n",
    "load_full = False\n",
    "\n",
    "if load_full:\n",
    "    data = Preprocess_Full_Dataset()\n",
    "    \n",
    "else:\n",
    "    data = Preprocess_Sample_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set column data types\n",
    "data[\"loan_status\"] = data[\"loan_status\"].astype('bool')\n",
    "data[\"loan_subgrade\"] = data[\"loan_subgrade\"].astype('int')\n",
    "data[\"description\"] = data[\"description\"].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summarize nulls/NaNs in data columns\n",
    "# FIX - print only cols with nulls\n",
    "print\n",
    "print \"COUNT OF NULLS IN DATA SET BY COLUMN:\\n\"\n",
    "print data.isnull().sum()\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print dataset shape, and widen pandas dataframe display\n",
    "n, p = data.shape\n",
    "pd.set_option('display.max_columns', p)\n",
    "\n",
    "print\n",
    "print \"The shape of the data is\", data.shape\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display top rows of data set\n",
    "print\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# summarize data set\n",
    "print\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract loan description and status, and create n-grams from description\n",
    "\n",
    "# extract and pre-process loan description and loan_status for NLP\n",
    "data_nlp = data[[\"description\", \"loan_status\"]].copy()\n",
    "data_nlp[\"description\"] = data_nlp[\"description\"].str.replace(\"Borrower.* > \", \"\")\n",
    "n, p = data_nlp.shape\n",
    "rows_without_nans_flags = data_nlp[\"description\"].notnull()\n",
    "rows_without_nans_indexes = np.where(rows_without_nans_flags)\n",
    "data_nlp = data_nlp.ix[rows_without_nans_indexes[0], :]\n",
    "\n",
    "# create n-grams from loan description\n",
    "# TO DO: stem words\n",
    "vectorizer = CountVectorizer(stop_words = 'english', ngram_range = (3, 3))\n",
    "desc_matrix = vectorizer.fit_transform(data_nlp['description'].values)\n",
    "n, p = desc_matrix.shape\n",
    "\n",
    "# print descriptive information about n-grams\n",
    "feature_names = vectorizer.get_feature_names().reshape(-1, 1)\n",
    "print \"Number of descriptions and terms:\", n, p\n",
    "print\n",
    "print \"Sample description terms:\\n\", feature_names[:10, 0]\n",
    "print\n",
    "\n",
    "score_accum = 0\n",
    "num_iters = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot histogram of term frequencies\n",
    "plt.histogram(desc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute most frequent description terms associated with each loan status\n",
    "\n",
    "# split term matrix into defaulted vs. fully repaid\n",
    "default_term_matrix = desc_matrix[data_nlp[\"loan_status\"] == False]\n",
    "repaid_term_matrix = desc_matrix[data_nlp[\"loan_status\"] == True]\n",
    "\n",
    "# calculate each term frequency\n",
    "default_term_freqs = default_term_matrix.sum().reshape(-1, 1)\n",
    "repaid_term_freqs = repaid_term_matrix.sum().reshape(-1, 1)\n",
    "\n",
    "# rename columns\n",
    "default_term_freqs.columns = [\"term_frequencies\"]\n",
    "repaid_term_freqs.columns = [\"term_frequencies\"]\n",
    "\n",
    "# combine term frequencies with feature names as a reverse dictionary\n",
    "default_term_dict = pd.concat((feature_names, default_term_freqs))\n",
    "repaid_term_dict = pd.concat((feature_names, repaid_term_freqs))\n",
    "\n",
    "# sort dictionaries by term frequency\n",
    "default_term_dict = default_term_dict.sort(columns = \"term_frequencies\", ascending = False)\n",
    "repaid_term_dict = repaid_term_dict.sort(columns = \"term_frequencies\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Most Frequent Terms in Descriptions of Defaulted Loans:\"\n",
    "default_term_dict.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Most Frequent Terms in Descriptions of Fully Repaid Loans:\"\n",
    "repaid_term_dict.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use description column, n-grams, and KNN to predict defaults and score accuracy\n",
    "\n",
    "# use KNN with cross-validation to train and score models predicting loan defaults\n",
    "# TO DO: tune parameters k for KNN and n for n-grams\n",
    "for i in range(num_iters):\n",
    "    mask = np.random.rand(n) < 0.8\n",
    "\n",
    "    train_y = data_nlp[\"loan_status\"][mask]\n",
    "    test_y = data_nlp[\"loan_status\"][~mask]\n",
    "\n",
    "    train_x = desc_matrix[mask]\n",
    "    test_x = desc_matrix[~mask]  \n",
    "\n",
    "    model = KNN(n_neighbors = 20)\n",
    "    model.fit(train_x, train_y)\n",
    "    score_accum += model.score(test_x, test_y)\n",
    "\n",
    "# print prediction accuracy\n",
    "score = score_accum / float(num_iters)\n",
    "print \"Accuracy of predicting defaults from descriptions with KNN:\", round(score, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use description column with logistic regression to predict defaults and score accuracy\n",
    "\n",
    "# use logistic regression with cross-validation to train and score models\n",
    "# predicting loan defaults\n",
    "for i in range(num_iters):\n",
    "    mask = np.random.rand(n) < 0.8\n",
    "\n",
    "    train_y = data_nlp[\"loan_status\"][mask]\n",
    "    test_y = data_nlp[\"loan_status\"][~mask]\n",
    "\n",
    "    train_x = desc_matrix[mask]\n",
    "    test_x = desc_matrix[~mask]  \n",
    "\n",
    "    model = Log_Reg()\n",
    "    model.fit(train_x, train_y)\n",
    "    score_accum += model.score(test_x, test_y)\n",
    "\n",
    "# print prediction accuracy\n",
    "score = score_accum / float(num_iters)\n",
    "print \"Accuracy of predicting defaults from descriptions with logistic regression:\", \\\n",
    "        round(score, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use description column with LDA to predict defaults and score accuracy\n",
    "\n",
    "# use LDA with cross-validation to train and score models\n",
    "# predicting loan defaults\n",
    "for i in range(num_iters):\n",
    "    mask = np.random.rand(n) < 0.8\n",
    "\n",
    "    train_y = data_nlp[\"loan_status\"][mask]\n",
    "    test_y = data_nlp[\"loan_status\"][~mask]\n",
    "\n",
    "    train_x = desc_matrix[mask]\n",
    "    test_x = desc_matrix[~mask]  \n",
    "\n",
    "    model = LDA()\n",
    "    model.fit(train_x, train_y)\n",
    "    score_accum += model.score(test_x, test_y)\n",
    "\n",
    "# print prediction accuracy\n",
    "score = score_accum / float(num_iters)\n",
    "print \"Accuracy of predicting defaults from descriptions with LDA:\", round(score, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use description column with QDA to predict defaults and score accuracy\n",
    "\n",
    "# use QDA with cross-validation to train and score models\n",
    "# predicting loan defaults\n",
    "for i in range(num_iters):\n",
    "    mask = np.random.rand(n) < 0.8\n",
    "\n",
    "    train_y = data_nlp[\"loan_status\"][mask]\n",
    "    test_y = data_nlp[\"loan_status\"][~mask]\n",
    "\n",
    "    train_x = desc_matrix[mask]\n",
    "    test_x = desc_matrix[~mask]  \n",
    "\n",
    "    model = QDA()\n",
    "    model.fit(train_x, train_y)\n",
    "    score_accum += model.score(test_x, test_y)\n",
    "\n",
    "# print prediction accuracy\n",
    "score = score_accum / float(num_iters)\n",
    "print \"Accuracy of predicting defaults from descriptions with QDA:\", round(score, 2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
