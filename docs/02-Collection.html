<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Predicting Loan Outcomes</title>
  <link rel="stylesheet" type="text/css" href="cs109a.css">
</head>
<body>
<h1>Predicting Loan Outcomes using Machine Learning</h1>
  <div class='ournames'>
    <h3>David Modjeska and Andrew Greene</h3>
    <h4>CS109a: Introduction to Data Science<br>
    Harvard University, Autumn 2016</h4>
  </div>

<ul style="margin-bottom: 2%">
<li><a href='01-Context.html'>1. Initial Context</a></li>
<li><a href='02-Collection.html'>2. Data Collection</a></li>
<li><a href='03-Exploration.html'>3. Data Exploration</a></li>
<li><a href='04-Modeling.html'>4. Data Modeling</a></li>
<li><a href='05-Analysis.html'>5. Modeling Analysis</a></li>
</ul>

<h2>Part 2: Data Collection</h2>

<p>In this part, we discuss some issues with the raw data provided by Lending Club, as well as the need to acquire additional data for context.</p>

<h3>Economic conditions</h3>

<p>The data in the Lending Club dataset covers the period 2008--2015, which includes the global <a href="https://fred.stlouisfed.org/series/JHDUSRGDPBR">&ldquo;Great Recession&rdquo; of 2007 Q4--2009 Q2.</a> In order to account for the variation in economic conditions, we felt it necessary to collect additional data reflecting economic conditions, and join that with the LC data. We used the Federal Reserve&#39;s &ldquo;FRED&rdquo; database, and in our presentation we will discuss this process.</p>

<p>We will also discuss the role that selection bias plays in the raw LC data. In particular, the provided dataset only includes loans that were approved by Lending Club members.</p>

<p>We note that the loan default rate differs over time:</p>

<p><img src="images/image04.png" /></p>

<p>In theory, a naive model would regard the issue date of the loan (issue_d) as a useful predictor. Naturally, however, we are not interested in predicting the past. We determined that the impact of the issue date was likely related to economic conditions. Therefore we augmented our data set with data from <a  href="https://fred.stlouisfed.org/">the Federal Reserve of St. Louis&#39;s &ldquo;FRED&rdquo; database</a>. Three major metrics of economic conditions were selected for the popularity, availability, and ability to provide context for the Lending Club data itself. These quarterly metrics are consumer price index (CPI), gross domestic product (GDP), and unemployment rate. The metrics were aligned with the quarter in which a LC loan was issued, for prediction purposes. </p>

<h3>Selection bias</h3>

<p>The data provided by Lending Club covers loans that were actually funded by Lending Club members. As such, this data omits loan applications for which loans were not actually issued. This omission poses several difficulties in our analysis:</p>

<ul><li>We cannot improve any model&rsquo;s score by improving the model&rsquo;s recall. That is, our model could potentially be valuable in recommending approval for loans that were not actually underwritten by Lending Club members, but which would have been paid in full. Because such loans are by definition absent from the LC data set, this path is not available for our research. (I.e., false negatives are not available for analysis.)</li></ul>

<ul><li>Those loans which Lending Club members correctly declined to fund are also absent from the data set. As a consequence of this absence, our model-building cannot identify common properties of these unfunded loans.</li></ul>

<p>In essence, then, the problem before us is not to build a general-purpose model that can analyze a new loan application and decide whether to fund it. Rather, the problem is to act like an iteration of a boosting algorithm -- taking only those loans which the Lending Club membership has decided to fund, and finding the edge cases where that human &ldquo;decision ensemble&rdquo; has failed. (I.e., the problem is to find false positives.)</p>

</body>
</html>
